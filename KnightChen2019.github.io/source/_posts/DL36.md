---
title: 这！就是深度学习之机器学习基础(三十六)
top: false
cover: false
toc: true
mathjax: true
date: 2019-08-11 17:35:25
password:
summary: 莫听穿林打叶声，何妨吟啸且徐行。竹杖芒鞋轻胜马，谁怕？一蓑烟雨任平生。料峭春风吹酒醒，微冷，山头斜照却相迎。回首向来萧瑟处，归去，也无风雨也无晴。
tags:
  - 极大似然估计
categories: 深度学习
---
首先更新一下机器学习算法的思维导图：
![机器学习算法](dl3601.jpg)
以前已经说过了的，就不再赘述，只是对部分的知识点，深挖一下，这次就先说极大似然估计（Maximum Likelihood Estimation）。

##### 极大似然估计的目的
利用已知样本结果，反推出最有可能（最大概率）导致这样结果的参数值。

##### 极大似然估计的原理
基于统计学中的概率论，通过若干次试验，观察其结果，利用实验结果得到某个参数值能够使样本出现的概率最大。

###### 1. 似然函数
假设样本集中的样本都是独立同分布，记为：D={$x_1, \cdots,x_N$}, 则其概率密度函数$p\left(D|\theta\right)$称为在给定数据D的情况下，参数θ的似然函数：
$$l(θ)=p(D|θ)=p(x_1, \cdots,x_N)=\prod_{i=1}^{N}p(x_i|θ)$$
如果$\hat{\theta}$是使得似然函数l(θ)最大的θ值，则称θ为极大似然函数估计值，即
$$\hat{\theta}=\underset{\theta}{argmax}l(\theta)=\underset{\theta}{argmax}\prod_{i=1}^{N}p(x_i|θ)$$

###### 2. 对数似然函数
为了便于求导，我们一般取对数似然函数$Inl\left(\theta\right)$，这样可以把乘法转换成加法，而且不改变它的单调性，我们假设新的对数似然函数为H(θ)则：
$$H(θ)=Inl(θ)=ln\prod_{i=1}^{N}p(x_i|θ)=\sum_{i=1}^{N}lnp(x_i|θ)$$

###### 3. 求解
令其导数为零，$\frac{d}{dx}H(θ)=0$

##### 极大似然估计的应用：
要想知道极大似然估计可以用在哪里，我们关键要知道概率密度函数$p\left(D|\theta\right)$有哪些，举2个常见的例子。

###### 1. 高斯分布
假设样本服从$N(\mu,\sigma^{2})$, 则高斯分布为：
$$N(x|\mu,\sigma^{2})=\frac{1}{\sqrt{2\pi}\sigma}exp \left (\frac{1}{2\sigma^{2}}\left (x-\mu \right)^{2}\right )$$
其似然函数为：
$$L(\mu,\sigma^{2})= \prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}exp \left (\frac{1}{2\sigma^{2}}\left (x-\mu \right)^{2}\right )$$
其对数似然函数为：
$$InL(\mu,\sigma^{2}) = In\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}exp \left (\frac{1}{2\sigma^{2}}\left (x-\mu \right)^{2}\right ) $$ 
$$ = \sum_{i=1}^NIn\frac{1}{\sqrt{2\pi}\sigma}exp \left (\frac{1}{2\sigma^{2}}\left (x-\mu \right)^{2}\right ) $$
$$ = \sum_{i=1}^NIn\frac{1}{\sqrt{2\pi}\sigma} + \sum_{i=1}^N-\frac{\left (x-\mu \right)^2}{2\sigma^2}$$
$$ = -\frac{n}{2}In(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^N(x-\mu)^2$$
$$ = -\frac{n}{2}In(2\pi)  -\frac{n}{2}In(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^N(x-\mu)^2 $$
要求似然函数最大值，分别对$\mu,\sigma^2$求偏导，并且令导数为零：
$$\left\{\begin{matrix}
\frac{h(x)}{d\mu} = 0 + \frac{1}{\sigma^2}\sum_{i=1}^n(x_{i}-\mu) = 0 \\ 
\frac{h(x)}{d\sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^n(x_{i}-\mu)^2 = 0 
\end{matrix}\right.$$
从而求得$\mu,\sigma^2$为：
$$\left\{\begin{matrix}
\mu = \bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i \\ 
\sigma^{2} = \frac{1}{n}\sum_{i=1}^{n}(x_i-\bar {x})^2
\end{matrix}\right.$$

似然函数有唯一解$(\mu ,\sigma^{2})$, 所以它就是最大值点。
 
###### 2. 伯努利（Bernoulli）分布
伯努利分布是一个离散型机率分布。试验成功，随机变量取值为1；试验失败，随机变量取值为0。成功机率为p，失败机率为q =1-p，所以伯努利分布又称两点分布。
Bernoulli分布的密度函数为：
$$Ber(x|θ)=\theta^x(1-\theta)^{1-x}$$
其对数似然函数为：
$$l(θ)=\sum_{i=1}^{N}logBer(x_i|\theta)$$
$$ =\sum_{i=1}^{N}log(\theta^x(1-\theta)^{1-x})) $$
$$ =N_1log\theta + N_2log(1-\theta) $$

其中$N_1$为实验结果为1的次数， $N_2$为实验结果为0的次数：
$$\left\{\begin{matrix}
N_1 = \sum_{i=1}^{N}x_i \\ 
N_2 = \sum_{i=1}^{N}(1-x_i) 
\end{matrix}\right.$$

求导可得：
$$\frac {dl(\theta)}{d\theta} = \frac{N_1}{\theta} - \frac{N_2}{1-\theta} = 0$$
$$\Rightarrow \hat{\theta} = \frac {N_1}{N_1+N_2} = \frac {N_1}{N}$$

逻辑回归的本质其实就是伯努利分布下的极大似然估计。



