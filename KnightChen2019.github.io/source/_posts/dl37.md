---
title: 这！就是深度学习之机器学习基础(三十七)
top: false
cover: false
toc: true
mathjax: true
date: 2019-08-19 01:57:24
password:
summary: 以慢为快
tags:
  - SVM
  - KKT
categories: 机器学习
---


支持向量机主要用于二分类的问题，是一种监督学习算法。
比如说对于这样的一个分类问题：
![SVM](dl3701.jpg)

我们期望找到一个超平面$w^Tx+b=0$,把样本可以正确的分类出来。离超平面最近的样本点称为支持向量，所以分类的原则就是支持向量所在的平行平面间隔最远。

##### 支持向量机求解
现在假设经过支持向量的平面分别为$w^Tx+b=C_1$和$w^Tx+b=-C_2$，因为是平行的平面，那么我们分别除以常数$C_1$和$C_2$,可得新的平面为$\frac{w^T}{C_1}x+\frac{b}{C_1}=1$和$\frac{w^T}{C_2}x+\frac{b}{C_2}=-1$, 既然系数都是常数,那么我们也可以写成新的$w^Tx+b=1$和$w^Tx+b=-1$。那么我们现在计算两个平面间隔的距离。
首先任意一个点到超平面的距离为$\gamma = \frac{\vert w^Tx+b+0 \vert}{\Vert w \Vert}$,其中w为法向量，我们现在要求最小的$\gamma$, 由上面的图可以看出来，对于任何一个正例样本点，都有$w^T+b \geq 1$, 对于任意一个负例样本点，都有$w^T+b \leq 1$, 加上绝对值之后，最小的距离就是1了，所以最近点的，即支持向量到超平面的距离就是$\gamma = \frac{1}{\Vert w \Vert}$，要让两个平面的间隔最大，就转化为了求$max \frac{2}{\Vert w \Vert}$ ，就让分子最小化：
$$\underset {w,b}{min} \frac{1}{2} \ {\Vert w \Vert}^2$$
$$s.t. y_i(w^Tx_i + b) \geq 1, i=1,2, \cdots,N$$

前面加了$\frac{1}{2}$, 只是为了后面求导可以方面一些。

对于上面不等式约束的优化问题，首先我们引入拉格朗日函数，因为拉格朗日函数可以把有约束的优化问题变为无约束的优化问题。定义拉格朗日乘子为$\lambda_i, \lambda_i \geq 0$，则拉格朗日函数为：
$$L(w,b,\lambda) = \frac{1}{2} {\Vert w \Vert}^2 + \sum_{i=1}^{N} \lambda_i (1- y_i(w^Tx_i + b))$$
这样就相当于原始的约束条件消掉了, 融进了新的拉格朗日函数。
通过上式可以看到，如果对于任意一个样本点不符合约束$y_i(w^Tx_i + b) < 1$,则$(1- y_i(w^Tx_i + b)) >0$, 又因为$\lambda_i \geq 0$, 所以$L(w,b,\lambda) = \frac{1}{2}  {\Vert w \Vert}^2 + \infty = \infty$,没有解。
如果所有样本点符合约束，同样的道理，$L(w,b,\lambda)$就有极大值$\frac{1}{2} {\Vert w \Vert}^2$
这就是我们期望的, 也就是我们现在的问题就是求解
$$\underset{w,b}{min} \underset{\lambda_i \geq 0}{max} L(w,b,\lambda)$$
因为是一个凸二次规划问题，满足强对偶性，所以极大极小可以转为极小极大问题：
$$\underset{\lambda_i \geq 0}{max} \underset{w,b}{min}  L(w,b,\lambda)$$

首先求$\underset{w,b}{min}L(w,b,\lambda)$, 分别求w和b的偏导数，并且令偏导数为零即可求得最小值。
$$\frac{\partial L(w,b,\lambda)}{\partial w} = w - \sum_{i=1}^N \lambda_i y_i x_i = 0\  \Rightarrow w=\sum_{i=1}^N \lambda_i y_i x_i$$
$$\frac{\partial L(w,b,\lambda)}{\partial b} = 0 + \sum_{i=1}^N \lambda_i y_i=0\  \Rightarrow \sum_{i=1}^N \lambda_i y_i =0$$

把上式中的w和b带回拉格朗日函数$L(w,b,\lambda)$ :
$$L(w,b,\lambda) =\frac{1}{2}w^Tw+\sum_{i=1}^N(\lambda_i-\lambda_i y_i w^T x_i + \lambda_i y_i b) 
$$ 
$$=\frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \lambda_i \lambda_j y_i y_j (x_i \cdot x_j) + \sum_{i=1}^N \lambda_i -\sum_{i=1}^N \sum_{j=1}^N \lambda_i \lambda_j y_i y_j(x_i \cdot x_j) + \sum_{i=1}^N \lambda_i y_i b
$$
因为$\sum_{i=1}^N \lambda_i y_i =0$, 所以上式：
$$= \sum_{j=i}^N \lambda_i - \frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \lambda_i \lambda_j y_i y_j (x_i \cdot x_j)
$$

接着求$\underset{\lambda_i \geq 0}{max} L(w,b,\lambda)$, 此时改变一下符号可变为求最小值：
$$\underset{\lambda_i \geq 0}{min} \ \frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \lambda_i \lambda_j y_i y_j (x_i \cdot x_j) -\sum_{j=i}^N \lambda_i $$
$$s.t. \sum_{i=1}^N \lambda_i y_i =0$$
$$\lambda_i \geq 0, i =1, 2, \cdots,N$$

当求出最大的$\lambda_i$，也就求出了w，b，从而也就确定了超平面的函数：
$$w^Tx+b = \sum_{i=1}^N \lambda_iy_i x_i^Tx+b$$
需要满足的KKT条件是:
$$\begin{cases}
\lambda_i \geq 0, i= 1,2,\cdots,N\\
y_i(w^Tx+b)-1 \geq 0\\
\lambda_i(y_i(w^Tx+b)-1) = 0\\
\end{cases}
$$
通过上式，可以看到，如果$\lambda_i >0$,则必有$y_i(w^Tx+b)-1=0$,此时对应的样本就是支持向量。

对于$\lambda_i$的求法，因为一次算出所有的$\lambda_i$值不太容易，我们可以用SMO（Sequential Minimal Optimization, 序列最小化）算法求得最大的$\lambda_i$.

SMO算法就是一次比较2个$\lambda_i$的值，将其他的变量都视为常数，每次保留其中最大的值，依次比较，从而求出最大的$\lambda_i$。

##### 软间隔
因为在实际情况中，一个面把所有点都100%的分割出来是比较难的，所以我们允许一些误差出现，此时引入松弛向量$\xi$，我们称之为软间隔，则原始问题就变成了：
$$\underset {w,b}{min} \frac{1}{2} \ {\Vert w \Vert}^2 + C \sum_{i=1}^N \xi_i$$
$$s.t. \  y_i(w^Tx_i + b) \geq 1-\xi_i, i=1,2, \cdots,N$$
$$\xi_i \geq 0, i = 1,2, \cdots,N $$

C是常数，表示犯错误的容忍程度，C越大，表示容忍度越低，即过渡带会越小。反之，越大。

求解方式也是类似的，引入拉格朗日乘子$\alpha_i,\mu$，转化为拉格朗日函数：
$$L(w,b,\xi,\alpha,\mu) = \frac{1}{2} {\Vert w \Vert}^2 + C \sum_{i=1}^N \xi_i + \sum_{i=1}^{N} \alpha_i (1-\xi_i- y_i(w^Tx_i + b)) - \mu \xi_i$$
分别求偏导可得：
$$\frac{\partial L(w,b,\xi,\alpha,\mu)}{\partial w} =0 \Rightarrow w=\sum_{i=1}^N \lambda_i y_i x_i$$
$$\frac{\partial L(w,b,\xi,\alpha,\mu)}{\partial b} = 0 \Rightarrow \sum_{i=1}^N \lambda_i y_i =0$$
$$\frac{\partial L(w,b,\xi,\alpha,\mu)}{\partial \xi} = 0 \Rightarrow C- \alpha_i - \mu_i = 0$$
把上式带入原函数,可得:
$$L(w,b,\xi,\alpha,\mu)=-\frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) +\sum_{j=i}^N \lambda_i$$
现在的问题就变成了：
$$\underset{\alpha}{max} -\frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) +\sum_{j=i}^N \alpha_i$$
$$s.t. \ \sum_{i=1}^N \alpha_i y_i =0$$
$$C-\alpha_i-\mu_i = 0$$
$$\alpha_i \geq 0, i = 1,2,\cdots,N$$
$$\mu_i \geq 0, i = 1,2,\cdots,N$$
因为$0 \leq \alpha_i \geq C$, 同时求最大变为求最小，所以上式可变为：
$$\underset{\alpha}{min} \ \frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) -\sum_{j=i}^N \alpha_i$$
$$s.t. \ C-\alpha_i-\mu_i = 0$$
$$0 \leq \alpha_i \geq C, i = 1,2,\cdots,N$$

此时的KKT条件为：
$$\begin{cases}
\lambda_i \geq 0, \mu_i \geq 0, i= 1,2,\cdots,N \\ 
y_i(w^Tx+b)-1+\xi_i \geq 0 \\ 
\lambda_i(y_i(w^Tx+b)-1+\xi_i) = 0 \\ 
\xi_i \geq 0, \mu_i \xi_i =0,i= 1,2,\cdots,N \\ 
\end{cases}
$$

##### 一般情况下的优化方式：
1 无约束优化问题，可以写为:
$min f(x)$
优化方法：根据费马大定律，求f(x)的导数，并且令导数为0.

2 有等式约束的优化问题，可以写为:
$min f(x)$
$s.t. h_i(x) = 0; i =1, …, n$
优化方法： 拉格朗日乘子法，通过拉格朗日函数把等式约束与原函数合为以个函数表示$L(x, \alpha) = f(x) + \alpha h_i(x)$, 然后对各个变量求导，令其为零，在结果集合中选取最优值。

3 有不等式约束的优化问题，可以写为：
$min f(x)$
$s.t. g_i(x) \leq 0, i =1,2,\cdots, n$
$h_i(x) = 0, i =1,2,\cdots, m$
优化方法： KKT条件。同样利用拉格朗日函数把所有的优化条件和原函数合为一个函数。$L(x,\alpha,\lambda) = f(x) + \lambda g_i(x) + \alpha h_i(x)$,此时最优值必须满足以下条件：
$$\begin{cases}
L(x,\alpha,\lambda) \text {对}x \text{求导为0} \\ 
h_i(x) = 0 \\ 
\lambda g_i(x) = 0 \\ 
\end{cases}
$$

