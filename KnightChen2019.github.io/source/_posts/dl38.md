---
title: 这！就是深度学习之机器学习基础(三十八)
top: false
cover: false
toc: true
mathjax: true
date: 2019-08-19 01:57:36
password:
summary: 稳扎稳打
tags:
  - PCA
  - LDA
categories: 机器学习
---
  这次主要讲降维的二种方法：主成分分析和线性判别分析的公式推导。

##### 主成分分析（Principal Component Analysis，简称PCA）
  是一种降维方式，属于无监督学习。举个例子，预测房价的时候，我们会有许多特征，比如房子的长，宽，高，面积，体积，等等。但是这些特征中，房子面积可以由长乘宽得到，房子体积可以由长乘宽乘高得到，所以面积和体积这些特征，和长，宽，高这些特征是线性相关的，所以PCA就是通过去除线性相关来达到降维的目的。
  
  PCA要找的是主成分的轴的方向，根据的是最大投影方差和最小重构距离。最大投影方差可以理解为将样本点投影到主轴之后，分散的最开，即方差最大。最小重构误差，则是要求投影的距离最小。举个简单的例子，如下图所示：
 ![二维降一维](dl3801.jpg)
  在二维坐标系中，所有样本点在$u_1$方向，不管是投影之后的误差，还是投影的距离，都要比$u_2$好，这里的好就体现在2个方面，即样本点在$u_1$投影之后尽可能的分散开了，并且样本点到$u_1$的距离很近，所以我们认为$u_1$就是降维的主成分方向。
  
  
  不管用哪种方法，都要首先对数据进行中心化，就是把数据平移，从而使所有的数据中心为0，这样，当找到一个特征向量a=[1,2], 主成分的方向就是从原点出发，到点(1,2)的向量方向，否则难以界定何为主方向。

###### 基于最大投影方差
![向量投影](dl3802.jpg)
假设有n个中心化之后的样本$X=\{x_1,x_2,\cdots,x_n\}$, 此时的主方向所在的向量为$\vec u$, 并且u为单位向量，即$\left \| u\right \|=1$，如上图所示，对于其中任意一个样本点来说，则投影在u向量之后的向量为
$$\vec x \cdot \vec u$$
用矩阵表示就是：
$$x^Tu$$
因为已经中心化了，所以它的方差为:
$$\left \| x^Tu \right \|^2 = (x^Tu)^T(x^Tu)=(u^Tx)(x^Tu)$$
又因为矩阵的乘法结合律，所以上式
$$=u^T(xx^T)u$$
对于所有的样本点来说，它们的方差和为：
$$\frac{1}{n} \sum_{i=1}^n u^T(x_ix_i^T)u$$
$$=u^T (\frac{1}{n}  \sum_{i=1}^n (x_ix_i^T) )u$$
同时令$S=\frac{1}{n}  \sum_{i=1}^n (x_ix_i^T)$, 因为已经中心化了，所以这里的$x_i=x_i - \overline x$, 其实S就是协方差矩阵。
所以上式可以变为求
$$arcmax \sum_{i=1}^Nu^TSu$$
$$s.t. \left \| u\right \|=1$$
这是一个等式约束， 所以构造拉格朗日函数：
$$L(u,\lambda)=u^TSu +\lambda (1- \left \| u\right \|)$$
$$=u^TSu + \lambda (1- u^Tu)$$
然后对u求导：
$$\frac{\partial L}{\partial u} =2Su-\lambda 2u=0 $$
$$\Rightarrow Su = \lambda u$$
对于上式，其实就是特征值和特征向量的定义， $\lambda$是特征值组成的对角阵，$u$是特征向量组成的矩阵。特征值按照从大到小排列，前k个特征值对应的特征向量组成的矩阵即为我们降到k维对应的解。

###### 基于最小投影距离（最小重构距离）
假设有n个中心化之后的样本$X=\{x_1,x_2,\cdots,x_n\}$, 此时的主方向所在的向量为$\vec u$, 并且u为单位向量，即$\left \| u\right \|=1$, 对于其中任意一个样本点来说，则投影的向量$\vec e$为：
$$\vec e = \vec x - (\vec x \cdot \vec u)\vec u$$
用矩阵表示就是：
$$\vec e=x-(x^Tu)u$$

那模长就是：
$$\left \| \vec e\right \|^2 = e^Te=[x-(x^Tu)u]^T[x-(x^Tu)u]$$
这里的$x^Tu$是向量的点积，得到的结果为一个实数，所以上式可以变为：
$$=[x^T-(x^Tu)u^T][x-(x^Tu)u]$$
$$=x^Tx-(x^Tu)u^Tx-(x^Tu)x^Tu+(x^Tu)^2 u^Tu$$
$$=x^Tx-(x^Tu)x^Tu-(x^Tu)x^Tu+(x^Tu)^2$$
$$=x^Tx-(x^Tu)(x^Tu)-(x^Tu)(x^Tu)+(x^Tu)^2$$
$$=x^Tx-(x^Tu)^2-(x^Tu)^2+(x^Tu)^2$$
$$=\left \| x\right \|^2-(x^Tu)^2$$
因为$\left \| x\right \|^2$是固定值，所以我们要求上式的最小值，即求$(x^Tu)^2$的最大值。
又因为矩阵乘法的结合律
$$(x^Tu)^2=(x^Tu)(x^Tu)$$
$$=(u^Tx)(x^Tu)$$
$$=u^T(xx^T)u$$
刚才求的是一个样本点的距离，那对于所有的样本来说，我们现在的目标就是:
$$max \sum_{i=1}^Nu^T(x_ix_i^T)u = arcmax \sum_{i=1}^Nu^T(X)u$$
$$s.t. \left \| u\right \|=1$$
这个问题就变成了一个等式约束的优化问题，那我们构造拉格朗日函数：
$$L(u,\lambda)=u^TXu + \lambda (1- \left \| u\right \|)$$
$$=u^TXu + \lambda (1- u^Tu)$$
然后对u求导：
$$\frac{\partial L}{\partial u} =Xu-\lambda u=0 $$
$$\Rightarrow Xu = \lambda u$$

可以看到和最大投影方差的解的形式是一样的。

##### 线性判别分析法（Linear Discriminant Analysis, 简称LDA）
是一种降维方式，属于监督学习。LDA希望选取一个投影方向，能够最大化的分离出原有类别。即期望类间样本方差小，不同类之间中心距离要大，如下图所示：
![LDA](dl3803.jpg)

假设样本共K类，每一类的样本个数为$N_1, N_2, \cdots,N_k$, 则第k类表示为$x_k^1,x_k^2,\cdots,x_k^k$。
设$\tilde {x_i^j}$为${x_i^j}$投影后的坐标，则：
$$\tilde {x_i^j}=<x,u>u = (x^Tu)u$$
向量u为投影的方向，不一定为单位向量，可设为$\left \| \vec u\right \|^2 = u^Tu = a$, $\tilde m$为均值
则第k类的样本平方和为：
$$S_k=\sum_{\tilde {x} \in D_k} (\tilde x - \tilde m)^T(\tilde x - \tilde m)$$
$$=\sum_{x \in D_k} ((x^Tu)u-(m^Tu)u)^T ((x^Tu)u-(m^Tu)u)$$
$$=\sum_{x \in D_k} ((x^Tu)u^T-(m^Tu)u^T) ((x^Tu)u-(m^Tu)u)$$
$$=\sum_{x \in D_k} (x^Tu)^2u^Tu - (m^Tu)(x^Tu)u^Tu -(x^Tu)(m^Tu)u^Tu+(m^Tu)^2u^Tu$$
$$=a\sum_{x \in D_k}((x^Tu)^2 - 2(x^Tu)(m^Tu) + (m^Tu)^2)$$

样本方差为上式除以样本个数：
$$\frac {S_k} {N_k} =a (\frac {\sum_{x \in D_k} (x^Tu)^2}{N_k} - 2 \frac{\sum_{x \in D_k} x^T(um^Tu)}{N_k} +\frac {\sum_{x \in D_k} (m^Tu)^2} {N_k})$$
$$=a(\frac{\sum_{x \in D_k} (u^Tx)(x^Tu)}{N_k} -2 \frac {\sum_{x \in D_k} x^T} {N_k} (um^Tu) +(m^Tu)^2)$$
$$=a(u^T \frac{\sum_{x \in D_k} xx^T}{N_k}u -2m^T(um^Tu) + (m^Tu)^2)$$
$$=a(u^T \frac{\sum_{x \in D_k} xx^T}{N_k}u -2(m^Tu)(m^Tu) + (m^Tu)^2)$$
$$=a(u^T \frac{\sum_{x \in D_k} xx^T}{N_k}u - (m^Tu)^2)$$
$$=a(u^T \frac{\sum_{x \in D_k} xx^T}{N_k}u - (u^Tm)(m^Tu))$$
$$=a(u^T \frac{\sum_{x \in D_k} xx^T}{N_k}u - u^T(mm^T)u)$$
$$=au^T(\frac{\sum_{x \in D_k} xx^T}{N_k} - mm^T)u$$

上面某一类的样本方差，那么所有类别的样本方差总和就是：
$$\sum_{k=1}^K S_k =a \sum_{k=1}^K u^T(\frac {\sum_{k=1}^K x_k x_k^T}{N_k} - m^km_k^T)u$$
$$=au^T(\sum_{k=1}^K \frac {\sum_{k=1}^K x_k x_k^T}{N_k} - m^km_k^T)u$$
令
$$S_w=(\sum_{k=1}^K \frac {\sum_{k=1}^K x_k x_k^T}{N_k} - m^km_k^T)$$
可以看到$S_w$其实就是样本协方差矩阵，则上式可以表示为：
$$au^T S_w u$$
我们期望上式可以最小。

假设$\tilde {m_i}$表示投影之后的第i类样本的均值，$\tilde {m_j}$表示投影之后的第j类样本的均值，则不同类别之间的中心距离为：
$$S_{ij}=(\tilde {m_i} - \tilde {m_j})^T(\tilde {m_i} - \tilde {m_j})$$
$$=((m_i^Tu)u-(m_j^Tu)u)^T ((m_i^Tu)u-(m_j^Tu)u)$$
$$=((m_i^Tu)u^T-(m_j^Tu)u^T) ((m_i^Tu)u-(m_j^Tu)u)$$
$$=(m_i^Tu)^2u^Tu -(m_j^Tu)(m_i^Tu)u^Tu-(m_i^Tu)(m_j^Tu)u^Tu-(m_j^Tu)^2u^Tu$$
$$=u^T((m_i^Tu)^2 - 2(m_i^Tu)(m_j^Tu) + (m_j^Tu)^2)u$$
$$=u^T((m_i^T)^2u^2-2m_i^Tm_j^Tu^2+(m_j^T)^2u^2)u$$
$$=u^2u^T((m_i^T)^2-2m_i^Tm_j^T+(m_j^T)^2)u$$
$$=u^2u^T(m_i^T-m_j^T)^2u$$
$$=u^2u^T(m_i^T-m_j^T)^T(m_i^T-m_j^T)u$$
$$=u^2u^T(m_i-m_j)(m_i-m_j)^Tu$$
对于所有不同类的中心距离之和为：
$$\sum_{i \neq j}^K S_{ij}= \sum_{i \neq j}^K u^2u^T(m_i-m_j)(m_i-m_j)^Tu$$
$$=u^2u^T(\sum_{i \neq j}^K (m_i-m_j)(m_i-m_j)^T)u$$
令$S_b=(\sum_{i \neq j}^K (m_i-m_j)(m_i-m_j)^T$,则上式可写为：
$$u^2u^TS_bu$$

我们令
$$J(u)=\frac{u^2u^TS_bu}{au^T S_w u}$$
因为前面说了$\left \| \vec u\right \|^2 = u^Tu = a$，所以上式变为：
$$J(u)=\frac{u^TS_bu}{u^T S_w u}$$
我们希望J(u)最大，也就是希望$u^TS_bU$最大，$u^T S_w u$最小，我们假定$u^T S_w u=1$,则我们的目标就变为了：
$$max u^TS_bu$$
$$s.t. \    u^T S_w u=1$$
有变成了一个等式约束，我们应当很熟悉了，利用拉格朗日乘子法求解，首先构造拉格朗日函数：
$$L(u, \lambda)= u^TS_bu + \lambda(1-u^T S_w u)$$
求导，令导数为零：
$$\frac {\partial L}{\partial u} = 2S_bu-\lambda 2S_wu=0$$
$$\Rightarrow S_bu =\lambda S_wu$$
$$\Rightarrow S_b S_w^{-1} u =\lambda u$$
令$T =S_b S_w^{-1}$,则上式变为
$$Tu =\lambda u$$
又变成了求解特征值和特征向量的问题。

##### PCA和LDA的区别
* LDA是有监督的降维方法，而PCA是无监督的降维方法
* LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。
* LDA除了可以用于降维，还可以用于分类。
* LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。这点可以从下图形象的看出，在某些数据分布下LDA比PCA降维较优。
![PCA和LDA的区别](dl3804.jpg)