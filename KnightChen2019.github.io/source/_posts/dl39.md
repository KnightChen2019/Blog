---
title: 这！就是深度学习之机器学习基础(三十九)
top: false
cover: false
toc: true
mathjax: true
date: 2019-08-25 12:56:51
password:
summary: 初听不识曲中意，再听已是曲中人。
tags: 
- 线性回归
- 逻辑回归
categories: 机器学习
---
再次回顾线性回归和逻辑回归，是为了可以把知识串起来，形成体系。

#### 线性回归

##### 模型公式

线性回归的模型如下：
$$
h_\theta(x_1,x_2,\cdots,x_n)=\theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \dots + \theta_nx_n = \sum_{i=0}^n\theta_ix_i
$$
上面的式子中，我们令 $x_0=1$ ，此时，$\theta = b$

用矩阵表示为：
$$
h_\theta(X)=\theta^TX
$$
其中，X为$m \times n$维矩阵，m为样本个数，n为每个样本的特征数。

一般情况下，样本的真实值和预测值之间有个误差$\epsilon$,表示为：
$$
Y = \theta^TX+\epsilon
$$
上式中，$\epsilon$是$m \times 1$维向量，代表m个样本相对于线性回归方程的上下浮动程度。$\epsilon$是独立同分布的，并且服从均值为0，方差为$\sigma^2$的正太分布。

##### 最小二乘法

根据上面的公式，对于每个样本来说：
$$
\epsilon^{(j)}=y^{(j)}-\theta^Tx^{(j)} , j \in (1,2,\cdots,m)
$$
因为$\epsilon$是独立同分布的，并且服从均值为0，方差为$\sigma^2$的正太分布，所以
$$
f(\epsilon^{(j)})=\frac {1} {\sqrt {2 \pi} \sigma}e^{-\frac {(\epsilon^{(j)})^2} {2 \sigma^2}}
$$

$$
=\frac {1} {\sqrt {2 \pi} \sigma}e^{-\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}}
$$

引入似然函数：
$$
L(\theta)=\prod_{j=1}^m f(y^{(j)}|x^{(j)};\theta)=\prod_{j=1}^m \frac {1} {\sqrt {2 \pi} \sigma}e^{-\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}}
$$
两边同时取对数，令$l(\theta) = logL(\theta)$,可得：
$$
l(\theta) = log\prod_{j=1}^m \frac {1} {\sqrt {2 \pi} \sigma}e^{-\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}}
$$

$$
=\sum_{j=1}^mlog \frac {1} {\sqrt {2 \pi} \sigma}e^{-\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}}
$$

$$
=\sum_{j=1}^mlog \frac{1}{\sqrt {2 \pi} \sigma} + \sum_{j=1}^mlog e^{-\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}}
$$

$$
=mlog \frac{1}{\sqrt {2 \pi} \sigma} + \sum_{j=1}^m -\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}
$$

$$
= mlog \frac{1}{\sqrt {2 \pi} \sigma} - \frac{1}{\sigma^2} \frac{1}{2} \sum_{j=1}^m (y^{(j)}-\theta^Tx^{(j)})^2
$$

上面的式子是求极大似然函数的最大值，但是去除常数项，和负号之后，它就转为了求
$$
J(\theta) = \frac{1}{2} \sum_{j=1}^m (y^{(j)}-\theta^Tx^{(j)})^2
$$
的最小值，这也就是最小二乘法的由来。

把上式用矩阵表示：
$$
J(\theta) = \frac{1}{2} (X\theta - Y)^T(X\theta - Y)
$$

$$
=\frac{1}{2}(\theta^TX^T-Y^T)(X\theta - Y)
$$

$$
=\frac{1}{2}(\theta^T X^T X \theta -Y^T X\theta -\theta^T X^T Y +Y^TY)
$$

对上式求导数：
$$
\bigtriangledown J(\theta) = \frac{1}{2}(2X^TX\theta - (Y^TX)^T -X^TY)
$$

$$
=\frac{1}{2}(2X^TX\theta -X^TY-X^TY)
$$

$$
=X^TX\theta -X^TY
$$

令$\bigtriangledown J(\theta) =0$,可得：
$$
X^TX\theta = X^TY
$$
此时，如果$X^TX$可逆，则方程有最优解：
$$
\theta = (X^TX)^{-1}X^TY
$$
如果$X^TX$不可逆，一般加入干扰项，正则化处理：
$$
\theta = (X^TX + \lambda I)^{-1}X^TY
$$

##### 梯度下降法

说完了最小二乘法的推导，我们再看看另外一种办法，梯度下降法的推导。

我们一般将损失函数写作（多加了个常数m，不影响结果）：
$$
J(\theta) = \frac{1}{2m} \sum_{j=1}^m (h_\theta(x^{(i)})- y^{(j)})^2
$$
直接对$\theta$求偏导：
$$
\frac{\partial}{\partial \theta_j} J(\theta) = \frac{\partial}{\partial \theta_j}\sum_{j=1}^m  \frac{1}{2m} (h_\theta(x^{(i)})- y^{(j)})^2
$$

$$
=\frac{1}{2m}2 \sum_{i=1}^m (h_\theta(x^{(i)})- y^{(j)}) \frac{\partial}{\partial \theta_j} \sum_{i=1}^m(\theta x^{(i)} - y^{(i)})
$$

$$
=\frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})- y^{(j)}) \frac{\partial}{\partial \theta_j} \sum_{i=1}^m(\theta x^{(i)} - y^i)
$$

$$
= \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})- y^{(j)}) \sum_{i=1}^m x_j^{(i)}
$$

$$
= \frac{1}{m} \sum_{i=1}^m ((h_\theta(x^{(i)})- y^{(j)}) x_j^{(i)})
$$

加入学习率$\alpha$,可得：
$$
\theta_i := \theta_i-\alpha \frac{\partial}{\partial \theta_j} J(\theta)
$$


然后我们可以选择具体选择哪种梯度下降法：

- **批量梯度下降法**是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新。
- **随机梯度下降法**不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。
- **小批量梯度下降**，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用 batch_size个样本来对参数进行更新。



#### 逻辑回归

##### 模型公式

它的模型公式如下：
$$
h_\theta(x) = g(\theta^Tx)=\frac{1}{1+e^{- \theta^TX}}
$$
而它又复合伯努利分布，所以：
$$
p(Y=y|x)= \begin{cases}
h_\theta(x) = g(f(x)) = \frac{1}{1+e^{- \theta^TX}}, y=1 \\ 
1- h_\theta(x) = 1 - g(f(x)) = \frac{e^{- \theta^TX}}{1+e^{- \theta^TX}}, y=0
\end{cases}
$$
对数损失函数的标准形式为：
$$
L(Y,P(Y|X)) = -log(Y|X)
$$
它的意思是指分类为Y的情况下，使P(Y|X)达到最大。若模型是用最大概率的分类来做预测的，而Y是代表分类为正确的分类，而P(Y|X)则是代表正确分类的概率，那对数取反就是P(Y|X)越大，损失函数就越小。P(Y|X)=1时，损失就降为0，不可能再低了。 

将逻辑回归的表达式带入对数损失函数中，可得：
$$
L(y,P(Y=y|X)= \begin{cases}
log(h_\theta(x)), y=1 \\ 
log(1-h_\theta(x)), y=0
\end{cases}
$$
化简之后可得最终的损失函数为：
$$
J(\theta) = - \frac{1}{m} \sum_{i=1}^m[y_ilog(h_\theta(x_i))+(1-y_i)log(1-h_\theta(x_i))]
$$

##### 梯度下降法

$$
\frac{\partial}{\partial \theta_j} J(\theta) = - \frac{1}{m} \sum_{i=1}^m(y^{(i)} \frac{1}{h_\theta(x^{(i)})} \frac{\partial h_\theta(x^{(i)})}{\partial \theta_j} - (1-y^{(i)}) \frac{1}{1-h_\theta(x^{(i)})} \frac{\partial h_\theta(x^{(i)})}{\partial \theta_j})
$$

$$
=- \frac {1}{m} \sum_{i=1}^m(y^{(i)} \frac{1}{g(\theta^Tx)}-(1-y^{(i)}) \frac{1}{1-g(\theta^Tx)}) \frac{\partial g(\theta^Tx)}{\partial \theta_j}
$$

因为:
$$
g(x) = \frac {1}{1+e^{-x}}
$$

$$
g'(x)=g(x)(1-g(x))
$$

所以上式继续化简可得：
$$
= - \frac{1}{m} \sum_{i=1}^m (y^{(i)}\frac{1}{g(\theta^Tx)}-(1-y^{(i)}) \frac{1}{1-g(\theta^Tx)}) g(\theta^Tx^{(i)})(1-g(\theta^Tx^{(i)})) x_j^{(i)}
$$

$$
=- \frac{1}{m} \sum_{i=1}^{m}(y^{(i)}(1-g(\theta^Tx^{(i)}))-(1-y^{(i)})g(\theta^Tx^{(i)}))x_j^{(i)}
$$

$$
=- \frac{1}{m} \sum_{i=1}^{m}(y^{(i)}-g(\theta^Tx^{(i)}))x_j^{(i)}
$$

$$
= \frac{1}{m} \sum_{i=1}^{m}(h_\theta(x^{(i)}) -y^{(i)})x_j^{(i)}
$$

是不是觉得和线性回归的有点类似，形式确实一样，但是$h_\theta(x)$是不一样的。

接下来采用合适的梯度下降策略，一次次的迭代直到找到最优解。