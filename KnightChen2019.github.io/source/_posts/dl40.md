---
title: 这！就是深度学习之反向传播算法(四十)
top: false
cover: false
toc: true
mathjax: true
date: 2019-08-25 13:22:06
password:
summary: 一身转战三千里，一剑曾当百万师。
tags:
- BP
- DFN
- MLP
categories: 深度学习
---
前面的第二十七和二十八篇简单的介绍了一下神经网络，不过对于其中的BP算法，还是有点不太理解，所以这次举两个例子，期望加深理解。

#### 反向传播（Back Propagation）算法
当我们使用前馈神经网络接受输入x并产生输出$\hat {y}$时，信息通过网络向前流动。输入x提供的初始信息，然后传播到每一层的隐藏单元，最终产生输出$\hat {y}$，这称之为前向传播（forward propagation）。在训练过程中，前向传播可以持续向前直到它产生一个标量代价函数$J( \theta )$。反向传播（back propagation）算法，简称BP算法，允许来自代价函数的信息通过网络向后流动，以便计算梯度。BP算法基于梯度下降的策略，以目标为负梯度方向对参数进行调整。

#### 抽象的例子
假设有下图所示的神经网络
![反向传播](dl4001.jpg)

假设X为$N*m$的矩阵，其中N为样本的个数，m为特征数。W为权重的矩阵，f为作
用在h上的激活函数，$\tilde b$为$b_1^T$后沿着行方向的扩展，那么根据前向传播算法，可得：

$out =Z_LW_{L+1}+\tilde b_{L+1}$
我们假设当前的损失函数为J，因为反向传播算法的目的是要求神经网络模型的输出相对于各个参数的梯度值，即求$\frac {\partial J}{\partial out}$.

我们把上面的假设稍微具化一下，令：
$$Z_L=\begin{pmatrix}
z_{11} & z_{12} & z_{13} \\ 
z_{21} & z_{22} & z_{23}
\end{pmatrix},
W_{L+1}=\begin{pmatrix}
w_{11} & w_{12} \\ 
w_{21} & w_{22} \\ 
w_{31} & w_{32}
\end{pmatrix},
\tilde b_{L+1}=\begin{pmatrix}
b_{1} & b_{1} \\ 
b_{2} & b_{2}
\end{pmatrix}$$
$$out=\begin{pmatrix}
o_{11} & o_{12} \\ 
o_{21} & o_{22}
\end{pmatrix}$$
则：
$$Z_L \times W_{L+1}=\begin{pmatrix}
z_{11}w_{11}+z_{12}w_{21}+z_{13}w_{31} & z_{11}w_{12}+z_{12}w_{22}+z_{13}w_{32} \\ 
z_{21}w_{11}+z_{22}w_{21}+z_{23}w_{31} & z_{21}w_{12}+z_{22}w_{22}+z_{23}w_{32}
\end{pmatrix}$$
所以：
$$o_{11} = z_{11}w_{11}+z_{12}w_{21}+z_{13}w_{31} +b_1$$
$$o_{12} = z_{11}w_{12}+z_{12}w_{22}+z_{13}w_{32} +b_2$$
$$o_{21} = z_{21}w_{11}+z_{22}w_{21}+z_{23}w_{31} +b_1$$
$$o_{22} = z_{21}w_{12}+z_{22}w_{22}+z_{23}w_{32} +b_2$$

##### 对w的参数求偏导
根据链式求导法则（J -> o -> w）有：
$$\frac {\partial J}{\partial w_{11}}=\frac {\partial J} {\partial o_{11}}z_{11} + \frac {\partial J} {\partial o_{21}}z_{21}$$
$$\frac {\partial J}{\partial w_{12}}=\frac {\partial J} {\partial o_{12}}z_{11} + \frac {\partial J} {\partial o_{22}}z_{21}$$
$$\frac {\partial J}{\partial w_{21}}=\frac {\partial J} {\partial o_{11}}z_{12} + \frac {\partial J} {\partial o_{21}}z_{22}$$
$$\frac {\partial J}{\partial w_{22}}=\frac {\partial J} {\partial o_{12}}z_{12} + \frac {\partial J} {\partial o_{22}}z_{22}$$
$$\frac {\partial J}{\partial w_{31}}=\frac {\partial J} {\partial o_{11}}z_{13} + \frac {\partial J} {\partial o_{21}}z_{23}$$
$$\frac {\partial J}{\partial w_{32}}=\frac {\partial J} {\partial o_{11}}z_{13} + \frac {\partial J} {\partial o_{22}}z_{23}$$
用矩阵表示就是：
$$\begin{pmatrix}
\frac {\partial J}{\partial w_{11}} & \frac {\partial J}{\partial w_{12}} \\ 
\frac {\partial J}{\partial w_{21}} & \frac {\partial J}{\partial w_{22}} \\ 
\frac {\partial J}{\partial w_{31}} & \frac {\partial J}{\partial w_{32}}
\end{pmatrix} = 
\begin{pmatrix}
z_{11} & z_{21} \\ 
z_{21} & z_{22} \\ 
z_{31} & z_{32}
\end{pmatrix}
\begin{pmatrix}
\frac {\partial J} {\partial o_{11}} & \frac {\partial J} {\partial o_{12}} \\ 
\frac {\partial J} {\partial o_{21}} & \frac {\partial J} {\partial o_{22}}
\end{pmatrix}$$
最左边的矩阵也叫雅克比（jacobian）矩阵。所以：
$$\frac {\partial J} {\partial W_{L+1}} = Z_L^T \frac {\partial J} {\partial out}$$

##### 对bias参数b求偏导
$$\frac {\partial J} {\partial b_1} = \frac {\partial J} {\partial o_{11}} + \frac {\partial J} {\partial o_{21}}$$
$$\frac {\partial J} {\partial b_2} = \frac {\partial J} {\partial o_{12}} + \frac {\partial J} {\partial o_{22}}$$
用矩阵表示：
$$(\frac {\partial J} {\partial b})^T = (\frac {\partial J} {\partial b_1} \   \frac {\partial J} {\partial b_2}) =(\frac {\partial J} {\partial o_{11}} + \frac {\partial J} {\partial o_{21}} \   \frac {\partial J} {\partial o_{12}} + \frac {\partial J} {\partial o_{22}})$$
也就是将$\frac {\partial J} {\partial out}$的每一行都加起来。

上面的例子中，我们求的是$W_{L+1}的梯度$，既然是反向传播，我们接下来就要求$W_L$的梯度，根据上面的公式，我们可以得到：
$$\frac {\partial J} {\partial W_L} = Z_{L-1}^T \frac {\partial J} {\partial h_L}$$
那么现在的问题是怎么求$\frac {\partial J} {\partial h_L}$呢？通过图片可以看到$h_L$经过激活函数的变换到了$Z_L$, 那么同样的根据链式求导法则，我们先求出$Z_L$的梯度，再求出激活函数$f_L$的梯度，也就求出了$h_L$的梯度，即$\frac {\partial J} {\partial h_L}$
##### 对Z求偏导
$$\frac {\partial J}{\partial z_{11}}=\frac {\partial J} {\partial o_{11}}w_{11} + \frac {\partial J} {\partial o_{12}}w_{12}$$
$$\frac {\partial J}{\partial z_{12}}=\frac {\partial J} {\partial o_{11}}w_{21} + \frac {\partial J} {\partial o_{12}}w_{22}$$
$$\frac {\partial J}{\partial z_{13}}=\frac {\partial J} {\partial o_{11}}w_{31} + \frac {\partial J} {\partial o_{12}}w_{32}$$
$$\frac {\partial J}{\partial z_{21}}=\frac {\partial J} {\partial o_{21}}w_{11} + \frac {\partial J} {\partial o_{12}}w_{12}$$
$$\frac {\partial J}{\partial z_{22}}=\frac {\partial J} {\partial o_{21}}w_{21} + \frac {\partial J} {\partial o_{22}}w_{22}$$
$$\frac {\partial J}{\partial z_{23}}=\frac {\partial J} {\partial o_{21}}w_{31} + \frac {\partial J} {\partial o_{22}}w_{32}$$
用矩阵表示：
$$\begin{pmatrix}
\frac {\partial J}{\partial z_{11}} & \frac {\partial J}{\partial z_{12}} & \frac {\partial J}{\partial z_{13}} \\ 
\frac {\partial J}{\partial z_{21}} & \frac {\partial J}{\partial z_{22}}& \frac {\partial J}{\partial z_{23}}
\end{pmatrix} =
\begin{pmatrix}
\frac {\partial J} {\partial o_{11}} & \frac {\partial J} {\partial o_{12}} \\ 
\frac {\partial J} {\partial o_{21}} & \frac {\partial J} {\partial o_{22}}
\end{pmatrix}
\begin{pmatrix}
w_{11} & w_{12} & w_{31} \\ 
w_{21} & w_{22} & w_{33}
\end{pmatrix}$$
即：
$$\frac {\partial J} {\partial Z_L} = \frac {\partial J} {\partial out} W_{L+1}^T $$

##### 对激活函数求偏导
因为$f_L$是作用于$h_L$的激活函数，所以$Z_L = f_L (h_L)$
###### 1 激活函数为sigmoid
$$Z_L = \frac {1} {1+e^{-h_L}}$$
复合函数求偏导：
$$\frac {\partial J} {\partial h_L} = \frac {\partial J} {\partial Z_L} \frac {\partial Z_L} {\partial h_L}$$
$$=\frac {\partial J} {\partial Z_L} \frac {e^{-h_L}} {(1+e^{-h_L})^2}$$
$$=\frac {\partial J} {\partial Z_L} \frac {1} {(1+e^{-h_L})}\frac {e^{-h_L}} {(1+e^{-h_L})}$$
$$=\frac {\partial J} {\partial Z_L} Z_L (1-Z_L)$$
###### 2 激活函数为sigmoid
$$Z_L = \frac {e^{h_L} - e^{-h_L}} {e^{h_L} + e^{-h_L}}$$
$$\frac {\partial J} {\partial h_L} = \frac {\partial J} {\partial Z_L} \frac {\partial Z_L} {\partial h_L}$$
因为$\frac {\partial Z_L} {\partial h_L}$求导比较复杂，所以单拎出来说。
令$a=e^{h_L}, b=e^{-h_L}$
根据复合函数求导公式：
$$(\frac {u} {v})' = \frac {u'v-uv'} {v^2}$$
再令$u=(a-b), v=(a+b)$, 则有：
$$\frac {\partial J} {\partial h_L} = \left (\frac {a-b} {a+b} \right )'$$
$$=\frac {(a-b)'(a+b)-(a-b)(a+b)'} {(a+b)^2}$$
而：
$$(a-b)'=(e^{h_L} - e^{-h_L})'=e^{h_L}-(-1)e^{-h_L}=e^{h_L}+e^{-h_L}=a+b$$
$$(a+b)'=(e^{h_L} +e^{-h_L})'=e^{h_L}+(-1)e^{-h_L}=e^{h_L}-e^{-h_L}=a-b$$
带回原式可得：
$$\frac {(a-b)'(a+b)-(a-b)(a+b)'} {(a+b)^2}$$
$$=\frac {(a+b)^2-(a-b)^2} {(a+b)^2}$$
$$=1-\frac {(a-b)^2} {(a+b)^2}$$
$$=1-\left (\frac {a-b} {a+b} \right )^2$$
$$=1-\left (\frac {e^{h_L} - e^{-h_L}} {e^{h_L} + e^{-h_L}} \right )^2$$
所以
$$\frac {\partial J} {\partial h_L} = \frac {\partial J} {\partial Z_L} (1-\left (\frac {e^{h_L} - e^{-h_L}} {e^{h_L} + e^{-h_L}} \right )^2)$$
$$= \frac {\partial J} {\partial Z_L} (1-Z_L^2)$$

###### 3 激活函数为ReLU
$$Z_L=ReLU(h_L)=\left\{\begin{matrix}
0,\ h_L \leq 0 \\ 
h_L,\ h_L >0
\end{matrix}\right.$$
对$h_L$求偏导可得：
$$\frac {\partial J} {\partial h_L} = \frac {\partial J} {\partial Z_L} \frac {\partial Z_L} {\partial h_L}=\left\{\begin{matrix}
0,\ h_L \leq 0 \\ 
\frac {\partial J} {\partial Z_L},\ h_L >0
\end{matrix}\right.$$

#### 形象的例子
再举个形象点的例子，如下图：
![反向传播](dl4002.jpg)
权重和偏执已经在图中标出来了，我们假设$x_1=1,x_2=4,x_3=5$, 最后的真实结果$t_1=0.1,t_2=0.05$

##### 1 输入层到隐藏层
$$z_{h_1}=w_1x_1+w_3x_2+w_5x_3+b_1$$
$$z_{h_2}=w_2x_1+w_4x_2+w_6x_3+b_1$$
$$h1 = \sigma(z_{h_1})$$
$$h2 = \sigma(z_{h_2})$$

##### 2 隐藏层到输出层
$$z_{o_1}=w_7h_1+w_9h_2+b_2$$
$$z_{o_2}=w_8h_1+w_10h_2+b_2$$
$$o1 = \sigma(z_{o_1})$$
$$o2 = \sigma(z_{o_2})$$

##### 3 计算hidden layer 和 output layer的值
这里的激活函数用的是sigmoid。
$$z_{h_1}=w_1x_1+w_3x_2+w_5x_3+b_1=0.1 \times 1 + 0.3 \times 4 + 0.5 \times 5 + 0.5 = 4.3 $$
$$h1 = \sigma(z_{h_1}) = \sigma (4.3) = 0.9866$$
$$z_{h_2}=w_2x_1+w_4x_2+w_6x_3+b_1=0.2 \times 1 + 0.4 \times 4 + 0.6 \times 5 + 0.5 = 5.3$$
$$h2 = \sigma(z_{h_2}) = \sigma (5.3) = 0.9950$$
$$z_{o_1}=w_7h_1+w_9h_2+b_2 = 0.7 \times 0.9866 + 0.9 \times 0.9950 + 0.5 = 2.0862$$
$$o1 = \sigma(z_{o_1}) = \sigma (2.0862) = 0.8896$$
$$z_{o_1}=w_7h_1+w_9h_2+b_2 = 0.8 \times 0.9866 + 0.1 \times 0.9950 + 0.5 = 1.3888$$
$$o1 = \sigma(z_{o_1}) = \sigma (1.3888) = 0.8004$$

##### 4 用计算误差
这里用平方和来计算损失值，记做E。
$$E =\frac{1}{2} \left (\ (o_1 - t_1)^2 +(o_2 - t_2)^2 \right )$$
所以有：
$$\frac {dE} {d_{o_1}} = o_1 - t_1$$
$$\frac {dE} {d_{o_2}} = o_2 - t_2$$

##### 5 反向传播之倒一层
因为
$$z_{o_1}=w_7h_1+w_9h_2+b_2$$
$$z_{o_2}=w_8h_1+w_10h_2+b_2$$
所以
$\frac {dz_{o_1}} {dw_7}=h_1,\frac {dz_{o_2}} {dw_8}=h_1,\frac {dz_{o_1}} {dw_9}=h_2,\frac {dz_{o_2}} {dw_10}=h_2,\frac {dz_{o_1}} {db_2}=1,\frac {dz_{o_2}} {db_2}=1$
根据链式求导法则：
$\frac {dE} {dw_7}=\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {dw_7} = (o_1 - t_1)(o_1(1-o_1))h_1$
$=(0.8896-0.1)(0.8896(1-0.8896))0.9866=0.0765$
同理可得：
$\frac {dE} {dw_8}=\frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {dw_8} =0.7504 \times 0.1598 \times 0.9866 = 0.1183$
$\frac {dE} {dw_9}=\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {dw_9} =0.7896 \times 0.0983 \times 0.9950 = 0.0772$
$\frac {dE} {dw_10}=\frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {dw_10} =0.7504 \times 0.1598 \times 0.9950 = 0.1193$
$\frac {dE} {db_2}=\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {db_2} + \frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {db_2}$
$=0.7896 \times 0.0983 \times 1 + 0.7504 \times 0.1598 \times 1 =0.1975$
##### 6 反向传播之倒二层
上面已经求出了$\frac {dE} {dw_7},\frac {dE} {dw_8},\frac {dE} {dw_9},\frac {dE} {dw_10},\frac {dE} {db_2}$,我们接着求 $\frac {dE} {dw_1},\frac {dE} {dw_2},\frac {dE} {dw_3},\frac {dE} {dw_4},\frac {dE} {dw_5},\frac {dE} {dw_6},\frac {dE} {db_1}$
根据链式求导法则：
$\frac {dE} {dw_1} = \frac {dE} {dh_1} \frac {dh_1} {dz_{h_1}} \frac {dz_{h_1}} {dw_1}$
在对$h_1$求导的时候，因为$h_1$同时作用于$o_1$和$o_2$，所以
$\frac {dE} {dh_1} =\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {dh_1} + \frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {dh_1}$
$=0.7896 \times 0.0983 \times 0.7 + 0.7504 \times 0.1598 \times 0.8 = 0.1502$
从而求得：
$\frac {dE} {dw_1} = 0.1502 \times 0.0132 \times 1 = 0.0020$
同理求得：
$\frac {dE} {dw_3} = \frac {dE} {dh_1} \frac {dh_1} {dz_{h_1}} \frac {dz_{h_1}} {dw_3} =0.1502 \times 0.0132 \times 4 = 0.0079$
$\frac {dE} {dw_5} = \frac {dE} {dh_1} \frac {dh_1} {dz_{h_1}} \frac {dz_{h_1}} {dw_5} =0.1502 \times 0.0132 \times 5 = 0.0099$
接着求$h_2$节点对应的权重值：
$\frac {dE} {dw_2} = \frac {dE} {dh_2} \frac {dh_2} {dz_{h_2}} \frac {dz_{h_2}} {dw_2}$
而：
$\frac {dE} {dh_2} =\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {dh_2} + \frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {dh_2}$
$=0.7896 \times 0.0983 \times 0.9 + 0.7504 \times 0.1598 \times 0.1 = 0.0818$
从而：
$\frac {dE} {dw_2} =0.0818 \times 0.0049\times 1 = 0.0004$

$\frac {dE} {dw_4}=\frac {dE} {dh_2} \frac {dh_2} {dz_{h_2}} \frac {dz_{h_2}} {dw_4} =0.0818 \times 0.0049 \times 4 = 0.0016$
$\frac {dE} {dw_6}=\frac {dE} {dh_2} \frac {dh_2} {dz_{h_2}} \frac {dz_{h_2}} {dw_6} =0.0818 \times 0.0049 \times 5 = 0.0020$

$\frac {dE} {db_1}=\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {dh_1} \frac {dh_1} {dz_{h_1}} \frac {dz_{h_1}} {db_1} + \frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {dh_2} \frac {dh_2} {dz_{h_2}} \frac {dz_{2_1}} {db_1}$
$= 0.7896 \times 0.9983 \times 0.7 \times 0.0132 \times 1+ \times 0.7504 \times 0.1598 \times 0.1 \times 0.0049 \times 1= 0.0008$

然后我们开始利用梯度下降法求解，令学习率$\alpha=0.01$, 则有：
$w_1:=w_1-\alpha \frac {dE} {dw_1} = 0.1 - 0.01 \times 0.0020 = 0.1000$
$w_2:=w_2-\alpha \frac {dE} {dw_2} = 0.2 - 0.01 \times 0.0004 = 0.2000$
$w_3:=w_3-\alpha \frac {dE} {dw_3} = 0.3 - 0.01 \times 0.0079 = 0.2999$
$w_4:=w_4-\alpha \frac {dE} {dw_4} = 0.4 - 0.01 \times 0.0016 = 0.4000$
$w_5:=w_5-\alpha \frac {dE} {dw_5} = 0.5 - 0.01 \times 0.0099 = 0.4999$
$w_6:=w_6-\alpha \frac {dE} {dw_6} = 0.6 - 0.01 \times 0.0020 = 0.6000$
$w_7:=w_7-\alpha \frac {dE} {dw_7} = 0.7 - 0.01 \times 0.0765 = 0.6992$
$w_8:=w_8-\alpha \frac {dE} {dw_8} = 0.8 - 0.01 \times 0.1183 = 0.7988$
$w_9:=w_9-\alpha \frac {dE} {dw_9} = 0.9 - 0.01 \times 0.0772 = 0.8992$
$w_10:=w_10-\alpha \frac {dE} {dw_10} = 0.1 - 0.01 \times 0.1193 = 0.0988$
$b_1:=b_1-\alpha \frac {dE} {db_1} = 0.1 - 0.01 \times 0.0008 = 0.0500$
$b_1:=b_2-\alpha \frac {dE} {db_2} = 0.1 - 0.01 \times 0.1975 = 0.4980$

然后执行N次，直到找到下山的路，也就是误差降到最小值。

##### 7 Python实现上面的代码
![Python实现BP](dl4006.jpg)
可以看到大约1500次左右，已经收敛到最小误差了。

#### 激活函数
这部分，主要说几个常用的激活函数。
###### 1 sigmoid
![sigmoid](dl4003.jpg)
优点：
1 它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.
缺点：
1 在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。
###### 2 tanh
![tanh](dl4004.jpg)
优点：
1 它解决了Sigmoid函数的不是zero-centered输出问题。
2 它的导数包含了原函数的值, 所以多次求导的时候, 只需要预先算一次, 后面可以重复利用。
缺点：
1 梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。
###### 3 ReLU
![ReLU](dl4005.jpg)
优点：
1 解决了gradient vanishing问题 (在正区间)
2 计算速度非常快，只需要判断输入是否大于
3 收敛速度远快于sigmoid和tanh
缺点：
1 ReLU的输出不是zero-centered
2 Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。
###### 4 Softmax
公式为：
$$\sigma (z)_j= \frac {e^{z_j}} {\sum_{k=1}^K e^{z_k}}$$
将K维向量z, 映射到一个新的K维向量σ(z)上, 每个元素的值都∈(0,1),且加起来等于1, 且映射前后元素间的相对大小保持不变. 
$\sigma (z)_j$表示新向量的第 j 个元素, $z_j$表示原向量的第 j 个元素. 

一般与交叉熵损失函数搭配, 用于二分类, 多分类。
###### 5 Softplus
公式为：
$$\zeta = log(1+e^x)$$
平滑版的ReLU函数，用的较少。
###### 6 Maxout
公式为：
$$h_i(x)= \underset {j \in [1,k]}{max} z_{ij}$$
$$z_{ij}=x^TW_{...ij}+b_{ij}$$
比如说：
$$z_1=w_1X+b_1$$
$$z_2=w_2X+b_2$$
$$z_3=w_3X+b_3$$
$$out=max(z_1, z_2, z_3)$$
采用此种激活函数，计算参数会造成k倍增加。
