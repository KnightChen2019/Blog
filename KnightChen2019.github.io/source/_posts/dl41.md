---
title: 这！就是集成学习（四十一）
top: false
cover: false
toc: true
mathjax: true
date: 2019-09-01 18:36:55
password:
summary: 深秋帘幕千家雨,落日楼台一笛风。
tags:
  - Bagging
  - Boosting
  - Stacking
categories: 集成学习
---
集成学习（Ensemble Learning）就是用多个学习器合成一个强大的学习器来训练模型，从而提高模型的泛化能力。如果你听说过HA，它和HA有异曲同工之妙，本来一个server可以提供的服务，变成多个server来提供，提高了某种能力。

#### Bagging（Bootstrap Aggregation）

![Bagging](dl4101.jpg)

##### 过程：

1. 每一次都从原始样本集中抽取n个训练样本，然后放回，共进行k轮抽取，得到k个训练集。
2. 每次使用一个训练集训练模型，共得到k个模型。
3. 对于最终的结果，如果是分类问题，就采用投票的方法，如果是回归问题，就采用平均值的方法。
4. 用于减少方差。



##### 算法：

###### 随机森林（Random Forest）

使用CART决策树做为弱学习器的Bagging方法称为随机森林。特征随机性，采集和训练集样本数N一样个数的样本。



#### Boosting

![Boosting](dl4102.jpg)

##### 过程：

1. 先从初始训练集训练一个弱学习器。
2. 根据学习误差率更新训练样本的权重，使得误差率高的样本在训练集中得到更多的重视，再训练一个新的弱学习器。
3. 重复#2，直到弱学习器达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。
4. 对于最终的结果，如果是分类任务，按照权重进行投票，如果是回归任务，进行加权，再进行预测。
5. 用于减少偏差。



##### 算法：

###### AdaBoost（Adaptive boosting）

通过迭代每次学习一个基本分类器，每次迭代过程中，使用全部的样本，改变样本的权重，提高那些被前一轮弱分类器错误分类样本的权值，降低那些被正确分类样本的权值。最后，将基本分类器线性组合做为强分类器，其中给分类误差率小的弱分类器以大的权值，给分类错误率大的弱分类器以小的权值。



###### Boosting Tree

提升树模型采用加法模型与前向分步算法，同时基函数采用决策树算法，对待分类问题采用二叉分类树，对于回归问题采用二叉回归树。提升树模型可以看做是决策树的加法模型。



###### GBDT （Gradient Boost Decision Tree）

一方面可以从残差的角度来理解，每一颗回归树都是学习之前的树的残差（使用代价函数对上一轮训练出的模型函数f的偏导来拟合残差）。另一方面可以从梯度的角度掌握算法，即每一棵回归树通过梯度下降法学习之前树的梯度下降值。



###### XGBoost （eXtreme Gradient Boosting）

以分类回归树（CART树）进行组合。在XGBoost里，每棵树是一个一个往里添加的，每加一颗都是希望效果能够提升。为了限制叶子节点的个数，会在目标函数里加上一个惩罚项。



#### Stacking

![Stacking](dl4103.jpg)

学习几个不同的弱学习器，并通过一个元模型来组合它们，然后基于弱模型返回的多个预测结果输出最终的预测结果。

##### 过程：

1. 使用k折交叉验证法，弱学习器在k-1折数据上进行训练，并用剩下的1折进行预测。
2. 使用弱学习器的预测结果做为元模型的输入，从而预测出最终的预测结果。



集成学习也是一块蛮大的东西，这里只是开个头，没有细究算法的具体实现，毕竟只是听着名字，不知道意思，也不好。

