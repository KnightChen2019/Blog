---
title: 这！就是概率图模型之隐马尔可夫模型和条件随机场（四十三）
top: false
cover: false
toc: true
mathjax: true
date: 2019-09-15 15:08:15
password:
summary: 亭上秋风，记去年袅袅，曾到吾庐。
tags:
  - HMM
  - CRF 
categories: 概率图模型
---

在说条件随机场和隐马尔可夫模型之前，先说两个别的概念：判别式模型和生成式模型。

##### 判别式模型（Discriminative Model）

直接对条件概率$p(y|x;\theta)$建模，即在特征x出现的情况下标记y出现的概率。假设现在有一个分类问题，要根据动物的特征来区分大象（y=1）和狗（y=0），给定这样的一种数据集，回归模型比如逻辑回归，会师徒找到一条直线也就是决策边界，来区分大象和狗这两类，然后对于新来的样本，回归模型会根据这个新样本的特征去计算这个样本落在决策边界的哪一边，从而得到相应的分类结果。

常见的判别式模型有：

- 线性回归 （Linear Regression）
- 逻辑斯蒂回归（Logistic Regression）
- 神经网络（NN）
- 支持向量机（SVM）
- K近邻（KNN）
- 高斯过程（Gaussian Process）
- 条件随机场（Conditional Random Field，CRF）
- CART（Classification and Regression Tree）
- Boosting
- 线性判别分析法（LDA）



##### 生成式模型（Generative Model）

对x和y的联合分布$p(x,y)$建模，然后通过贝叶斯公式来求得$p(y_i|x)$，最后选取使得$p(y_i|x)$最大的$y_i$。比如还是想要区分大象和狗这个问题，首先根据训练集中的大象样本，建立大象的模型，根据训练集中狗的样本，再建立狗的模型。对于新来的样本，可以让它与大象模型匹配看概率有多少，再与狗模型匹配看概率有多少，哪一个概率大，就属于哪一类。

常见的生成式模型有：

- 朴素贝叶斯 （Naive Bayes）
- 高斯混合模型（GMM）
- 隐马尔可夫模型（HMM）
- 贝叶斯网络 （Bayesian Networks）
- Sigmoid Belief Networks
- 马尔可夫随机场（Markov Random Fields）
- 深度信念网络（DBN）
- 限制玻尔兹曼机



由上面的说明可以看到隐马尔可夫模型属于生成式模型，条件随机场属于判别式模型，接着说说这两种模型是个啥。

##### 隐马尔可夫模型（Hidden Markov Model，HMM）

隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。

###### 马尔可夫链（Markov Chain）

也称为离散时间马尔可夫链，因俄国数学家安德烈·马尔可夫得名，描述为状态空间中从一个状态到另一个状态转换的随机过程。该过程要求具备”无记忆“的性质：下一状态$S_{t+1}$的概率分布只能由当前状态$S_t$决定，与之前的状态无关，即：
$$
P(S_{t+1}|S_1,S_2,...,S_t)=P(S_{t+1}|S_t)
$$
这种特定类型的“无记忆性”称作马尔可夫性质，符合该性质的随机过程称为马尔可夫过程，也称为马尔可夫链。

比如下图中有两种状态A和B，如果我们在A，接下来可以过渡到B或留在A。如果我们在B，可以过渡到A或者留在B。

![马尔可夫链示意图](dl4301.JPG)



###### 隐马尔可夫模型的三要素

隐马尔可夫模型由初始状态概率向量π，状态转移概率矩阵A和观测概率矩阵B决定。π和A决定状态序列，B决定观测序列。因此，隐马尔可夫模型λ可以用三元符号表示，即：
$$
\lambda = (A,B,\pi)
$$
A,B,π称为隐马尔可夫模型的三要素。



举个例子，假设有4个盒子，每个盒子里都装有红白两种颜色的球，盒子里的红白球数如下表所示。

|  盒子  |  1   |  2   |  3   |  4   |
| :----: | :--: | :--: | :--: | :--: |
| 红球数 |  5   |  3   |  6   |  8   |
| 白球数 |  5   |  7   |  4   |  2   |

按照下面的方法抽球，产生一个球的颜色的观测序列：开始，从4个盒子里以等概率随机选取1个盒子，从这个盒子里随机抽出1个球，记录其颜色后，放回。然后，从当前盒子随机转移到下一个盒子，规则是：如果当前盒子是盒子1，那么下一个盒子一定是盒子2，如果当前是盒子2或者3，那么分别以概率0.4和0.6转移到左边或右边的盒子，如果当前是盒子4，那么各以0.5的概率停留在盒子4或转移到盒子3，确定转移的盒子后，再从这个盒子里随机抽出1个球，记录其颜色，放回。如此下去，重复进行5次，得到一个球的颜色的观测序列为：
$$
O=\{红，红，白，白，红\}
$$
在这个过程中，观察者只能观测到球的颜色的序列，观测不到球是从哪个盒子取出的，即观测不到盒子的序列。

在这个例子中有两个随机序列，一个是盒子的序列（状态序列），一个是球的颜色的观测序列（观测序列）。前者是隐藏的，只有后者是可观测的。根据条件，可以明确状态集合，观测集合，序列长度以及模型的三要素。

盒子对应状态，状态的集合是：
$$
Q=\{盒子1，盒子2，盒子3，盒子4\}， N=4
$$
球的颜色对应观测，观测的集合是：
$$
V = \{红，白\}， M=2
$$
状态序列和观测序列的长度$T=5$。

初始概率分布为：
$$
\pi=(0.25,0.25,0.25,0.25)^T
$$
状态转移概率分布为：
$$
A=\begin{bmatrix}
 0 & 1 & 0 & 0\\ 
 0.4 & 0 & 0.6 & 0\\ 
 0 & 0.4 & 0 & 0.6\\ 
 0 & 0 & 0.5 & 0.5
\end{bmatrix}
$$
观测概率分布为：
$$
B=\begin{bmatrix}
 0.5 & 0.5\\ 
 0.3 & 0.7\\ 
 0.6 & 0.4\\ 
 0.8 & 0.2
\end{bmatrix}
$$

###### 隐马尔可夫模型的两个基本假设

1. 齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态，于其他时刻的状态及观测无关，也与时刻t无关。

$$
P(i_t|i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(i_t|i_{t-1}), t=1,2,\cdots,T
$$

2. 观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关。	

$$
P(o_t|i_T,o_T,i_{T-1},o_{T-1},\cdots,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(o_t|i_t)
$$



###### 隐马尔可夫模型的三个基本问题

1. 概率计算问题。给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,\cdots,o_T)$，计算在模型λ下观测序列O出现的概率$P(O|\lambda)$。
2. 学习问题。已知观测序列$O=(o_1,o_2,\cdots,o_T)$，估计模型$\lambda=(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大。即用极大似然估计的方法估计参数。
3. 预测问题。也称为解码（decoding）问题。已知模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,\cdots,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,\cdots,i_T)$。即给定观测序列，求最有可能的对应的状态序列。



###### 隐马尔可夫模型的三个基本问题的解法

1. 概率计算问题。可以采用直接计算法，前向-后向（forward-backward algorithm）算法。
2. 学习问题。可以采用监督学习方法，鲍姆-韦尔奇（Baum-Welch）算法。
3. 预测问题。可以采用近似算法，维特比（Viterbi）算法。

具体的算法我就不细说了，如果以后用到了再详细梳理一下。



##### 条件随机场（Conditional Random Field，CRF）

条件随机场是给定随机变量X条件下，随机变量Y的马尔可夫随机场。



###### 马尔科夫随机场（Markov Random Field）

概率图模型（Probabilistic Graphical Model，PGM）是由图表示的概率分布。概率无向图模型（Probabilistic Undirected Graphical Model）又称为马尔可夫随机场，表示一个联合概率分布，其标准定义为：设有联合概率分布P（V）由无向图G=（V，E）表示，图G中的节点表示随机变量，边表示随机变量的依赖关系。如果联合概率分布P（V）满足成对，局部或全局马尔可夫性，就称此联合概率分布为概率无向图模型或马尔可夫随机场。

1. 成对马尔可夫性

   设无向图G中的任意两个没有边连接的节点u，v，其它所有节点为O，成对马尔可夫性指给定$Y_O$的条件下，$Y_u$和$Y_v$条件独立：$P(Y_u,Y_v|Y_O)=P(Y_u|Y_O)P(Y_v|Y_O)$。

   ![成对马尔可夫性](dl4302.JPG)

   

2. 局部马尔可夫性

   设无向图G的任一节点v，W是与v有边相连的所有节点，O是v，W外的其它所有节点，局部马尔可夫性指在给定$Y_W$的条件下，$Y_v$和$Y_O$条件独立：$P(Y_v,Y_O|Y_W)=P(Y_v|Y_W)P(Y_O|Y_W)$。

   当$P(Y_O|Y_W)>0$时，等价于$P(Y_v|Y_W)=P(Y_v|Y_W,Y_O)$。

   ![局部马尔可夫性](dl4303.JPG)

   

3. 全局马尔可夫性

   设节点集合A，B是在无向图G中被节点结合G分开的任意节点集合，全局马尔可夫性指给定$Y_C$的条件下，$Y_A$和$Y_B$条件独立，即：$P(Y_A,Y_B|Y_C)=P(Y_A|Y_C)P(Y_B|Y_C)$。

   ![全局马尔可夫性](dl4304.JPG)

   

###### 线性条件随机场（linear chain conditional random field）

这里主要介绍定义在线性链上的特殊的条件随机场，称为线性条件随机场（linear chain conditional random field）。

设$X=(X_1,X_2,\cdots,X_n),Y=(Y_1,Y_2,cdots,Y_n)$均为线性链表示的随机变量序列，若在给定随机变量序列X的条件下，随机变量序列Y的条件概率分布P（X|Y）构成条件随机场，即满足马尔可夫性：
$$
P(Y_i|X,Y_1,\cdots,Y_{i-1},Y{i+1},\cdots,Y_n)=P(Y_i|X,Y_{i-1},Y_{i+1})
$$

$$
i=1,2,\cdots,n(在i=1和n时只考虑单边)
$$

则称P（X|Y）为线性条件随机场。在标注问题中，X表示输入观测序列，Y表示对应的输出标记序列或状态序列。

![线性条件随机场](dl4305.JPG)



###### 条件随机场的概率计算问题

条件随机场的概率计算问题是给定条件随机场P（X|Y），输入序列x和输出序列y，计算条件概率$P(Y_i=y_i|x),P(Y_{i-1}=y_{i-1},Y_i=y_i|x)$以及相应的数学期望问题。

1. 前向-后向算法

   为了计算每个节点的概率，比如$P(Y=y_i|x)$的概率，对于这类概率，用前向或者后向算法其中的任何一个就可以解决。前向或后向算法，都是扫描一遍整体的边权值，计算P（X），只是它们扫描的方向不同，一个从后往前，一个从前往后。所以
   $$
   P(x)=Z(x)=\sum_yP(y,x)=\alpha_n^T(x)\cdot1=1^T \cdot \beta_1(x)
   $$
   上式中$\alpha$为前向向量，$\beta$为后向向量。

   根据前向-后向向量的定义，很容易计算标记序列在位置$i$是标记$y_i$的条件概率和在位置$i-1$与$i$是标记$y_{i-1}$和$y_i$的条件概率：
   $$
   P(Y_i=y_i|x)=\frac{\alpha_i^T(y_i|x) \beta_i(y_i|x)}{Z(x)}
   $$

   $$
   P(Y_{i-1}=y_{i-1},Y_i=y_i|x)=\frac{\alpha_{i-1}^T(y_{i-1}|x) M_i(y_{i-1},y_i|x) \beta_i(y_i|x)}{Z(x)}
   $$

   其中$Z(x)=\alpha_n^T(x) \cdot 1$

   

2. 计算期望

   利用前向-后向向量，可以计算特征函数关于联合分布P(X,Y)和条件分布P（Y|X）的数学期望。

   特征函数$f_k$关于条件分布P（Y|X）的数学期望是：
   $$
   E_{P(Y|X)}[f_k]=\sum_y P(y|x)f_k(y,x)
   $$

   $$
   =\sum_{i=1}^{n+1} \sum_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i) \frac{\alpha_{i-1}^T(y_{i-1}|x) M_i(y_{i-1},y_i|x) \beta_i(y_i|x)}{Z(x)},k=1,2,\cdots,K
   $$

   其中$Z(x)=\alpha_n^T(x) \cdot 1$



​	   假设经验分布为$\tilde{P}(X)$，特征函数$f_k$关于联合分布的数学期望是：
$$
E_{P(Y|X)}[f_k]=\sum_{x,y} P(x,y) \sum_{i=1}^{n+1} f_k(y_{i-1},y_i,x,i)
$$

$$
=\sum_x \tilde{P}(x) \sum_y P(y|x)\sum_{i=1}^{n+1} f_k(y_{i-1},y_i,x,i)
$$

$$
=\sum_x \tilde{P}(x) \sum_{i=1}^{n+1} \sum_{y_{i-1}y_i} f_k(y_{i-1},y_i,x,i) \frac{\alpha_{i-1}^T(y_{i-1}|x) M_i(y_{i-1},y_i|x) \beta_i(y_i|x)}{Z(x)},k=1,2,\cdots,K
$$

​		其中$Z(x)=\alpha_n^T(x) \cdot 1$



3. 参数学习算法

   条件随机场模型实际上是定义在时序数据上的对数线性模型，其学习方法包括极大似然估计和正则化的极大似然估计。具体的优化算法有改进的迭代尺度法IIS，梯度下降法以及拟牛顿法。

   假设经验分布为$\tilde{P}(X，Y)$，则训练数据的对数似然函数为：
   $$
   L(w)=L_{\tilde{p}} (P_w) = log \prod_{x,y}P_w(y|x)^{\tilde{p}(x,y)}=\sum_{x,y} \tilde{p}(x,y)logP_w(y|x)
   $$

4. 预测算法

   采用动态规划思想的维特比算法。

