---
title: 这！就是深度学习之循环神经网络（四十四）
top: false
cover: false
toc: true
mathjax: true
date: 2019-09-15 18:49:28
password:
summary: 今宵正对初弦月，傍水驿、深舣蒹葭。
tags:
  - RNN 
  - LSTM 
categories: 深度学习
---

循环神经网络（Recurrent Neural Network，RNN）是一类用于处理序列数据的神经网络。序列数据的特点就是前后之间有很强的关联性，也就是说前面出现的数据（比如词汇）对后面的数据有重大的影响，甚至后面的数据对前面的数据也是有重要影响的（双向循环神经网络）。例如句子，文档，语音都可以算是序列。

对于CNN来说，它的输出只考虑了前一个输入的影响而不考虑其它时刻输入的影响，对于猫，狗，手写数字等单个物体的识别具有较好的效果。但是，对于一些与时间先后有关的，比如视频的下一刻的预测，文档前后文内容的预测等等，这些算法的表现不尽人意，因此，RNN就应运而生了。

RNN不仅考虑前一刻的输入，而且对前面的内容有一种“记忆”功能。隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出。

##### RNN的结构

一个简单的RNN单元如下，左边是折叠式，右边是展开式：

![RNN结构](dl4401.JPG)

上图中，t-1，t，t+1表示时间序列，x表示输入的样本，$s_t$表示样本在t时刻的记忆，并且$s_t=f(W*S_{t-1}+U*X_t)$，W表示输入的权重，U表示此刻输入的样本的权重，V表示输出的样本权重。可以看到这里的权重W，U，V是共享的。



##### RNN的前向传播

在t=1时刻，一般初始化输入$s_0=0$，随机初始化W，U，V，进行下面的公式计算：
$$
h_1=Ux_1+Ws_0
$$

$$
s_1=f(h_1)
$$

$$
o_1=g(Vs_1)
$$

其中f和g均为激活函数，f可以是tanh，relu，sigmoid等激活函数，常用为tanh，g通常是softmax函数。

时间往前推进，此时的状态$s_1$做为时刻1的记忆状态将参加下一时刻的预测活动，也就是：
$$
h_2=Ux_2+Ws_1
$$

$$
s_2=f(h_2)
$$

$$
o_2=g(Vs_2)
$$

以此类推，可以得到最终的输出值为：
$$
h_t=Ux_t+Ws_{t-1}
$$

$$
s_t=f(h_t)
$$

$$
o_t=g(Vs_t)
$$

总结为一句话就是：新的输出是由新的输入和之前被记录下来的值所决定的。



##### RNN的反向传播

反向传播做的事情就是更新权重参数W，U，V，每一次的输出值$O_t$都会产生一个误差值$E_t$,则总的误差可以表示为：$E=\sum_{t}e_t$，损失函数可以使用交叉熵损失函数或者平方误差损失函数。

前面我们说过反向传播（Back Propagation ）算法，但是RNN除了依赖前一步的输出，还需要前若干步网络的状态，所以改进版的BP算法叫做Back Propagation Through Time（BPTT）算法。

这里我们选用激活函数tanh和softmax来做说明，并且输出用y表示之前的o，则：
$$
s_t=tanh(Ux_t + Ws_{t-1})
$$

$$
\hat y = softmax(Vs_t)
$$
我们定义损失函数为交叉熵损失，所以每一时刻的误差为：
$$
E_t(y_t,\hat y_t) = -y_t log \hat y_t
$$
总的误差为：
$$
E(y,\hat y)=\sum_t E_t(y_t,\hat y_t) = -\sum_t y_tlog \hat y_t
$$
这里的$y_t$是t时刻的正确单词，$\hat y_t$是网络的预测。比如，我们将完整的序列（句子）做为一个训练实例，总的误差则是各个时间点（单词）误差的和。

我们的目的是计算误差关于权重参数U，V和W的梯度，并通过梯度下降法（SGD）来更新权重参数。我们用$E_3$作为例子来推导。

首先看V的梯度：
$$
\frac{\partial E_3}{\partial V} = \frac{\partial E_3}{\partial \hat y_3} \frac{\partial \hat y_3}{\partial V} = \frac{\partial E_3}{\partial \hat y_3} \frac{\partial \hat y_3}{\partial z_3} \frac{\partial z_3}{\partial V} = (\hat y_3 - y_3) \bigotimes s_3
$$
上式中，我们定义$z_3 = Vs_3, \bigotimes$是两个向量的外积。因为已知$\hat y_3,y_3,s_3$,所以V的梯度可以直接计算出来。

接着看W的梯度：
$$
\frac{\partial E_3}{\partial W} = \frac{\partial E_3}{\partial \hat y_3} \frac{\partial \hat y_3}{\partial s_3} \frac{\partial s_3}{\partial W}
$$
其中$s_3=tanh(Ux_3+Ws_2)$,依赖于$s_2$.而$s_2$依赖于W和$s_1$，所以对于W的求导，需要用到链式法则，即：
$$
\frac{\partial E_3}{\partial W} = \sum_{k=0}^3 \frac{\partial E_3}{\partial \hat y_3} \frac{\partial \hat y_3}{\partial s_3} \frac{\partial s_3}{\partial s_k} \frac{\partial s_k}{\partial W}
$$
由于W在每时每刻都作用在我们所关心的输出上，所以我们需要从时刻t=3，通过网络的所有路径到时刻t=0来反向传播梯度，如下图所示：

![BPTT](dl4402.JPG)



同理可以求得U的梯度：
$$
\frac{\partial E_3}{\partial U} = \sum_{k=0}^3 \frac{\partial E_3}{\partial \hat y_3} \frac{\partial \hat y_3}{\partial s_3} \frac{\partial s_3}{\partial x_k} \frac{\partial x_k}{\partial U}
$$


##### 长短时记忆网络（Long Short Term Memory，LSTM）结构

当前预测位置和相关信息之间的文本间隔不断增大时，简单循环神经网络有可能丧失学习到距离如此远的信息的能力，或者在复杂语言场景中，有用信息的间隔有大有小，长短不一，循环神经网络的性能也会受到限制。

长短时记忆模型的设计就是为了解决这个问题。采用LSTM结构的循环神经网络比标准的循环神经网络表现更好。与单一tanh循环体结构不同，LSTM是一种拥有三个“门”结构的特殊网络结构。



###### 1. 遗忘门

遗忘门的结构如下：

![遗忘门](dl4403.JPG)

它决定丢掉细胞状态的哪些信息。根据$h_{t-1}$和$x_t$，遗忘门为状态$C_{t-1}$输出一个介于0到1之间的数字，0表示完全丢弃，1表示完全接受，数学表达式为：

$f_t=\sigma(W_f[h_{t-1},x_t]+b_f)$



###### 2.输入门

输入门的结构如下：

![输入门](dl4404.JPG)

它由两部分构成，第一部分为sigmoid函数，输出为$i_t$，决定更新哪些值，第二部分为tanh激活函数，输出为$\tilde {C_t}$，$i_t$和$\tilde {C_t}$相乘后的结构用于更新细胞状态，数学表达式为：

$i_t = \sigma(W_i[h_{t-1} ,x_t]+b_i)$

$\tilde {C_t} =tanh(W_C[h_{t-1},x_t]+b_C)$



###### 3 输出门

经过遗忘门换个输入门，细胞状态更新为：

![输出门01](dl4405.JPG)



输出基于上述细胞状态，但是需要过滤，输出门结构如下：

![输出门02](dl4406.JPG)

首先，我们使用sigmoid层决定输出细胞状态的哪些部分。然后，我们令细胞状态通过tanh层，输出结果与sigmoid层的输出结果相乘。数学公式为:

$o_t = \sigma(W_o[h_{t-1,x_t}]+b_o)$

$h_t=o_t*tanh(C_t)$



###### LSTM的前向传播算法

1. 更新遗忘门的输出：

   $f_t=\sigma(W_f[h_{t-1},x_t]+b_f)$

2. 更新输入门的输出：

   $i_t = \sigma(W_i[h_{t-1},x_t]+b_i)$

   $\tilde {C_t} =tanh(W_C[h_{t-1},x_t]+b_C)$

3. 更新细胞状态：

   $C_t = f_t*C_{t-1}+i_t* \tilde {C_t}$

4. 更新输出门的输出：

   $o_t = \sigma(W_o[h_{t-1,x_t}]+b_o)$

   $h_t=o_t*tanh(C_t)$$



##### 双向循环神经网络（BRNN）

对于语言建模来说，很多时候只看前面的词是不够的的，比如下面这句话：

> 我的手机坏了，我打算____一部新手机。

如果只看横线前面的词，手机坏了，我是打算修一修？哭一哭？还是买一部新的呢？这些是无法确定的，但是根据横线后面的词语“一部新手机”，那么大概率，我会“买”。

这个时候就需要双向循环神经网络，如下图所示：

![双向循环神经网络](dl4407.JPG)



##### 深度循环神经网络（DRNN）

前面提到的RNN都是只有一层隐藏层，如果将多个RNN单元堆叠在一起，那就形成了深度循环神经网络，或者叫多层RNN。它的网络结构如下：

![深度循环神经网络](dl4408.JPG)

























