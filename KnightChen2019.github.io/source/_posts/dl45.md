---
title: 机器学习模型中的一些评估指标（四十五）
top: false
cover: false
toc: true
mathjax: true
date: 2019-09-22 23:48:15
password:
summary: 人闲桂花落，夜静春山空。
tags: 
  - 模型评估指标
categories: 模型评估 
---


对于不同的学习任务，比如分类问题，回归问题，我们学得的模型是不同的。对于不同的模型，选择合适的指标，从而判断它们的性能，进而改进这些模型，提高学习效率。这一次，就聊聊这些指标。

#### 分类问题（Classification）

##### 1.四个名词

在介绍下面这些指标之前，我们先说4个基础的单词和它们缩写：

- True Positive（真正，TP）：将正类预测为正类数。
- True Negative（真负，TN）：将负类预测为负类数。
- False Positive（假正，FP）：将负类预测为正类数，误报（Type I error）。
- False Negative（假负，FN）：将正类预测为负类数，漏报（Type II error）。



##### 2. 混淆矩阵

混淆矩阵是对分类结果进行详细描述的一张表，主要用于比较分类结果和样本的信息。对于二分类问题，混淆矩阵如下表所示：

![混淆矩阵](dl4501.jpg)



##### 3. 准确率（Accuracy）

对于给定的测试数据集，分类器正确分类的样本总数（包含正负样本）与原始数据集总样本数之比。
$$
Accuracy = \frac {TP+TN} {TP+TN+FN+FP}
$$


> 如果正负样本不均衡，不能只用准确率做为评价指标。



##### 4. 精准率/查准率（Precision）

针对预测结果而言，预测为正类的样本（真正和假正）中，真的正类的比率。
$$
Precision = \frac {TP} {TP+FP}
$$


##### 5. 召回率/查全率  （Recall）

针对样本而言，被正确判定的正实例（TP）在总的正实例中（真正和假负）的比率。
$$
Recall = \frac {TP} {TP+FN}
$$


##### 6. F-measure（F1 Score）

一般而言，召回率高时，精准率低；精准率高时，召回率低。虽然Precision和Recall没有必然关系，但是它们通常被一起用来评估模型。F-measure就是Precision和Recall的加权调和平均数。

F-measure的一般形式：
$$
F = \frac {(1+\beta^2) \times P \times R}{\beta^2 \times P +R}
$$
其中$\beta$表示查全率与查准率的权重，如果查全率的权重=查准率的权重，即查全率和查准率一样重要，那么上述公式就变成了：
$$
F1 = \frac{2 \times Recall \times Precision}{Recall+Precision}
$$
也就是我们常说的F1度量，F1 Score。



##### 7. PR曲线

以精准率Precision作为纵轴，以召回率Recall作为横轴所画的曲线，如下图所示：

![PR曲线](dl4502.JPG)



PR曲线的具体绘制步骤是：

首先根据样本的置信度（confidence score）从大到小排序，这里置信度的意思是该样本是正样本的概率，比如90%的概率认为A样本是正例，10%的概率认为B样本是正例。

然后选取top-N（通常是top-5）的样本，预测为正例，剩下的样本预测为反例。

接着对每个样本计算Precision和Recall，就可以画出对应的曲线了。



###### AP计算

只是根据top-5来衡量一个模型的好坏，不太好，所以就有了另外一种叫做AP的计算方法，假设N个样本中有M个正例，那么我们会得到M个recall值（1/M, 2/M, ... ,M/M），对于每个recall值r，我们可以计算出对应的（r' >= r）的最大precision，然后对这M个precision值取平均，即得到最后的AP值。比如下图所示：

![AP计算](dl4503.JPG)



##### 8. 受试者工作特征曲线（Receiver Operating Characteristic curve，ROC）

以假正类率（False Positive Rate，FPR）为横轴，以真正类率（True Positive Rate，TPR）为纵轴所画的曲线。

假正类率：被模型判定为正类的负实例在实际的负样本中的比例。
$$
FPR = \frac{FP}{FP+TN}
$$
真正类率：被模型判定为正类的正实例在实际的正样本中的比例。
$$
TPR = \frac{TP}{TP+FN}
$$


ROC曲线上的每个点对应的是在某个阈值（threshold）下得到的（FPR，TPR）。设定一个阈值，大于这个阈值的实例被划分为正实例，小于这个值得实例则被划分为负实例，运行模型，得出结果，计算FPR和TPR的值。更换阈值，循环操作，就得到不同阈值下的（FPR，TPR），即可绘制成ROC曲线。并且ROC曲线在测试集中的正负样本的分布变化时，能够保持不变。



下图显示了一条ROC曲线：

![ROC曲线](dl4504.png)



ROC曲线与FPR围成的面积（AUC）越大，说明性能越好。一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting。



##### 9. AUC（Area Under Curve）

ROC曲线的面积，在0.5到1.0之间。之所以用AUC值做为评判标准，是因为很多时候不能从ROC曲线中判别模型的好坏。AUC的值越接近1，说明模型性能越好，模型预测的准确率越高。



#### 回归问题（Regression）

##### 1. 平均绝对误差（Mean Absolute Error，MAE）

绝对误差的平均值。

公式为：
$$
MAE = \frac{1}{m} \sum_{i=1}^m \left | h(x_i)-y_i \right |
$$


##### 2. 均方误差（Mean Square Error，MSE）

真实值与预测值的差值的平方，然后求和平均。常被用作线性回归的损失函数。

公式为：
$$
MSE = \frac {1}{m} \sum_{i=1}^m (y_i - \hat y_i)^2
$$
其中$y_i$表示真实值，$\hat y_i$表示预测值。



##### 3. 均方根误差 （Root Mean Square Error，RMSE）

衡量观测值与真实值之间的偏差。

公式为：
$$
RMSE = \sqrt {\frac {1}{m} \sum_{i=1}^m (h(x_i) - y_i)^2}
$$

##### 4.决定系数（Coefficient of determination）

也被称为$R^2$（R-Square）。

公式为：
$$
R2= 1 - \frac{\sum_i (\hat y_i-y_i)^2}{\sum_i (\bar y_i - y_i)^2}
$$
其中，分子部分表示真实值与预测值的平方差之和，类似于均方差MSE；分母部分表示真实值与均值的平方差之和，类似于方差Var。

R-square的取值范围为[0,1]：

如果结果是0，说明模型拟合效果很差；

如果结果是1，说明模型无错误。

一般来说，R-Square越大，表示模型拟合效果越好。